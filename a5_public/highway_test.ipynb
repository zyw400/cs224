{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Highway(nn.Module):\n",
    "    \"\"\"\n",
    "    The Highway Network as in https://arxiv.org/abs/1505.00387\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, e_word):\n",
    "        \"\"\"\n",
    "        Init the layers for the network\n",
    "        @param e_word (int): input size\n",
    "        \"\"\"\n",
    "        super(Highway, self).__init__()\n",
    "        self.h_projection = nn.Linear(e_word, e_word, bias = True)\n",
    "        self.h_gate = nn.Linear(e_word, e_word, bias = True)\n",
    "\n",
    "    def forward(self, x_conv_out):\n",
    "        \"\"\"\n",
    "        Looks up character-based CNN embeddings for the words in a batch of sentences.\n",
    "        @param x_conv_out: Tensor of shape (batch_size, e_word)\n",
    "\n",
    "        @param x_highway: Tensor of shape (batch_size, e_word)\n",
    "        \"\"\"\n",
    "\n",
    "        x_proj = F.relu(self.h_projection(x_conv_out))\n",
    "        x_gate = F.sigmoid(self.h_gate(x_conv_out))\n",
    "\n",
    "        x_highway = x_gate * x_proj + (1 - x_gate) * x_conv_out\n",
    "\n",
    "        return x_highway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2859, 0.8559, 0.4367],\n",
       "        [0.3571, 1.4384, 0.9562]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_word = 3\n",
    "a = Highway(3)\n",
    "input = torch.Tensor([[1,1,1],[2,2,2]])\n",
    "a(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiwzh-mac/opt/anaconda3/envs/local_nmt/lib/python3.5/site-packages/ipykernel_launcher.py:1: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/yiwzh-mac/opt/anaconda3/envs/local_nmt/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  \n",
      "/Users/yiwzh-mac/opt/anaconda3/envs/local_nmt/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/yiwzh-mac/opt/anaconda3/envs/local_nmt/lib/python3.5/site-packages/ipykernel_launcher.py:4: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.constant(a.h_projection.bias, 10)\n",
    "nn.init.constant(a.h_gate.bias, 10)\n",
    "nn.init.constant(a.h_projection.weight, 1)\n",
    "nn.init.constant(a.h_gate.weight, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13.0000, 13.0000, 13.0000],\n",
       "        [16.0000, 16.0000, 16.0000]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(torch.Tensor([10.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'int' and 'Highway'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-1c1c07b91f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'int' and 'Highway'"
     ]
    }
   ],
   "source": [
    "1-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(16, 33, 3, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 24])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The CNN module in the embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, e_char, e_word, k = 5, padding = 1):\n",
    "        \"\"\"\n",
    "        Init the layers for the network\n",
    "        @param e_char (int): input size\n",
    "        @param e_word (int): filter number, set to e_word\n",
    "        @param k (int): kernel size\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.projection = nn.Conv1d(e_char, e_word, k, padding = padding)\n",
    "\n",
    "    def forward(self, x_reshape):\n",
    "        \"\"\"\n",
    "        Calculate CNN layer outputs.\n",
    "        @param x_reshape: Tensor of shape (batch_size, e_char, m_word)\n",
    "\n",
    "        @param x_conv_out: Tensor of shape (batch_size, e_word)\n",
    "        \"\"\"\n",
    "\n",
    "        x_conv = self.projection(x_reshape)\n",
    "        x_conv_out = F.relu(x_conv).max(dim = -1).values.squeeze(-1)\n",
    "        \n",
    "        return x_conv_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_char = 3\n",
    "m_word = 2\n",
    "e_word = 4\n",
    "k = 2\n",
    "a = CNN(e_char,4, k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiwzh-mac/opt/anaconda3/envs/local_nmt/lib/python3.5/site-packages/ipykernel_launcher.py:1: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/yiwzh-mac/opt/anaconda3/envs/local_nmt/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[2., 2.],\n",
       "         [2., 2.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[2., 2.],\n",
       "         [2., 2.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[2., 2.],\n",
       "         [2., 2.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[2., 2.],\n",
       "         [2., 2.],\n",
       "         [2., 2.]]], requires_grad=True)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.constant(a.projection.bias, 10)\n",
    "nn.init.constant(a.projection.weight, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.Tensor([[[1,1],[2,2],[0,0]],[[1,1],[2,2],[1,1]]])\n",
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [2., 2.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[16., 22., 16.],\n",
       "         [16., 22., 16.],\n",
       "         [16., 22., 16.],\n",
       "         [16., 22., 16.]],\n",
       "\n",
       "        [[18., 26., 18.],\n",
       "         [18., 26., 18.],\n",
       "         [18., 26., 18.],\n",
       "         [18., 26., 18.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.projection(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22., 22., 22., 22.],\n",
       "        [26., 26., 26., 26.]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "s = nn.Embedding(400, 7, padding_idx=0)\n",
    "a = torch.LongTensor(np.array([[[2.0,3],[2,3],[0,1]],[[2,3],[2,3],[0,1]]]))\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 7, 2])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(a).permute(0,1,3,2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "CS224N 2019-20: Homework 5\n",
    "model_embeddings.py: Embeddings for the NMT model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "Anand Dhoot <anandd@stanford.edu>\n",
    "Michael Hahn <mhahn2@stanford.edu>\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Do not change these imports; your module names should be\n",
    "#   `CNN` in the file `cnn.py`\n",
    "#   `Highway` in the file `highway.py`\n",
    "# Uncomment the following two imports once you're ready to run part 1(j)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End \"do not change\"\n",
    "\n",
    "class ModelEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Class that converts input words to their CNN-based embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_embed_size, vocab, char_embed_size = 50, dropout_rate = 0.3):\n",
    "        \"\"\"\n",
    "        Init the Embedding layer for one language\n",
    "        @param word_embed_size (int): Embedding size (dimensionality) for the output word\n",
    "        @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.\n",
    "\n",
    "        Hints: - You may find len(self.vocab.char2id) useful when create the embedding\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(30, char_embed_size, padding_idx=0)\n",
    "        self.cnn = CNN(char_embed_size, word_embed_size)\n",
    "        self.highway = Highway(word_embed_size)\n",
    "        self.dropout = nn.Dropout(p = dropout_rate)\n",
    "        self.word_embed_size = word_embed_size\n",
    "\n",
    "        ### YOUR CODE HERE for part 1h\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Looks up character-based CNN embeddings for the words in a batch of sentences.\n",
    "        @param input: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where\n",
    "            each integer is an index into the character vocabulary\n",
    "\n",
    "        @param output: Tensor of shape (sentence_length, batch_size, word_embed_size), containing the\n",
    "            CNN-based embeddings for each word of the sentences in the batch\n",
    "        \"\"\"\n",
    "        x_emb = self.embedding(input)\n",
    "        x_reshaped = x_emb.permute(0,1,3,2)\n",
    "        x_conv_out = self.cnn(x_reshaped)\n",
    "        x_highway = self.highway(x_conv_out)\n",
    "        x_word_emb = self.dropout(x_highway)\n",
    "        return x_word_emb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ModelEmbeddings(6,{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_char = 50\n",
    "m_word = 5\n",
    "e_word = 6\n",
    "sentence_length = 2\n",
    "batch_size = 3\n",
    "input = torch.LongTensor([[[1,1,2,2,2],[2,2,2,2,2],[0,0,2,2,2]],[[1,1,2,2,2],[2,2,2,2,2],[1,1,2,2,2]]])\n",
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emb = m.embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reshaped = x_emb.permute(0,1,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emb = m.embedding(input)\n",
    "x_reshaped = x_emb.permute(0,1,3,2)\n",
    "\n",
    "dim_0 = x_reshaped.size()[0]\n",
    "dim_1 = x_reshaped.size()[1]\n",
    "\n",
    "x_reshaped_flattened = torch.flatten(x_reshaped, start_dim=0, end_dim=1)\n",
    "\n",
    "x_conv_out = m.cnn(x_reshaped_flattened)\n",
    "x_highway = m.highway(x_conv_out)\n",
    "x_word_emb = m.dropout(x_highway)\n",
    "\n",
    "x_word_emb_unflatten = x_word_emb.reshape(dim_0,dim_1,x_highway.size()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 6])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_word_emb_unflatten.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_highway.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.flatten(x_reshaped, start_dim=0, end_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = x_word_emb.reshape(dim_0,dim_0,x_highway.size()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]],\n",
       "\n",
       "         [[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]],\n",
       "\n",
       "         [[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]]],\n",
       "\n",
       "\n",
       "        [[[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]],\n",
       "\n",
       "         [[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]],\n",
       "\n",
       "         [[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 == x_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = nn.MaxPool1d(kernel_size=14 - 5 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22., 22., 22., 22.],\n",
       "        [26., 26., 26., 26.]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool1d(kernel_size = 2, input = a.projection(input)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22., 22., 22., 22.],\n",
       "        [26., 26., 26., 26.]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 2],\n",
       "        [2, 2, 2]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(F.relu(input), dim=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 2],\n",
       "        [2, 2, 2]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.relu(input).max(dim = -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[[1,1,1],[2,2,2],[0,0,2]],[[1,1,2],[2,2,2],[1,1,2]]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embed_size = 10\n",
    "word_embed_size = 5\n",
    "dropout_rate = 0.3\n",
    "embedding = nn.Embedding(30, char_embed_size, padding_idx=0)\n",
    "cnn = CNN(char_embed_size, word_embed_size)\n",
    "highway = Highway(word_embed_size)\n",
    "dropout = nn.Dropout(p = dropout_rate)\n",
    "word_embed_size = word_embed_size\n",
    "X_word_emb_list = []\n",
    "# print(\"input size: {}\".format(input.size()))\n",
    "# divide input into sentence_length batchs\n",
    "for X_padded in input:\n",
    "    # print(\"X_padded {}\".format(X_padded.size()))\n",
    "\n",
    "    X_emb = embedding(X_padded)\n",
    "    X_reshaped = torch.transpose(X_emb, dim0=-1, dim1=-2)\n",
    "    # conv1d can only take 3-dim mat as input\n",
    "    # so it needs to concat/stack all the embeddings of word\n",
    "    # after going through the network\n",
    "    # print(\"X_shaped {}\".format(X_reshaped.size()))\n",
    "    X_conv_out = cnn(X_reshaped)\n",
    "\n",
    "    X_highway = highway(X_conv_out)\n",
    "    X_word_emb = X_highway\n",
    "    X_word_emb_list.append(X_word_emb)\n",
    "\n",
    "x_word_emb_unflatten = torch.stack(X_word_emb_list)\n",
    "# return x_word_emb_unflatten\n",
    "### YOUR CODE HERE for part 1j\n",
    "# print(\"input: \")\n",
    "# print(input.size())\n",
    "batch_size, seq_len, max_word_length = input.shape[1], input.shape[0], input.shape[2]\n",
    "# print('batch size', batch_size)\n",
    "# print('max word length', max_word_length)\n",
    "#print('seq len', seq_len)\n",
    "\n",
    "x_char_embed = embedding(input)  # shape: (sentence_length, batch_size, max_word_length, e_char)\n",
    "#print('x_char embed shape', x_char_embed.shape)\n",
    "x_reshaped = x_char_embed.permute(0, 1, 3, 2)  # shape: (sentence_length, batch_size, e_char, max_word_length)\n",
    "#print('x_reshaped shape', x_reshaped.shape)\n",
    "x_conv = cnn(x_reshaped.view(-1, char_embed_size, max_word_length)) # shape (seq_len*batch_size, e_word)\n",
    "#print('x_conv shape', x_conv.shape)\n",
    "x_highway = highway(x_conv)  # shape: (batch_size*seq_len, e_word)\n",
    "#print('x_highway shape', x_highway.shape)\n",
    "x_word_embed2 = x_highway.view(seq_len, batch_size, word_embed_size)\n",
    "x_word_embed = dropout(x_highway.view(seq_len, batch_size, word_embed_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emb = embedding(input)\n",
    "x_reshaped = x_emb.permute(0,1,3,2)\n",
    "\n",
    "dim_0 = x_reshaped.size()[0]\n",
    "dim_1 = x_reshaped.size()[1]\n",
    "\n",
    "x_reshaped_flattened = torch.flatten(x_reshaped, start_dim=0, end_dim=1)\n",
    "\n",
    "x_conv_out = cnn(x_reshaped_flattened)\n",
    "x_highway = highway(x_conv_out)\n",
    "x_word_emb = x_highway\n",
    "\n",
    "x_word_emb_unflatten = x_word_emb.reshape(dim_0,dim_1,x_highway.size()[-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_word_embed = dropout(x_highway.view(seq_len, batch_size, word_embed_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1527, 0.3462, 0.0000, 0.0000, 0.3792],\n",
       "         [0.3134, 0.0000, 0.4361, 0.0000, 0.0000],\n",
       "         [0.1890, 0.1417, 0.0000, 0.0000, 0.0741]],\n",
       "\n",
       "        [[0.0000, 0.1770, 0.2188, 0.0000, 0.0202],\n",
       "         [0.0000, 0.1536, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2940, 0.1770, 0.0000, 0.0000, 0.0202]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_word_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1069, 0.2424, 0.1167, 0.0000, 0.2654],\n",
       "         [0.2194, 0.1075, 0.3053, 0.0000, 0.0000],\n",
       "         [0.1323, 0.0992, 0.0000, 0.0000, 0.0519]],\n",
       "\n",
       "        [[0.2058, 0.1239, 0.1531, 0.0000, 0.0141],\n",
       "         [0.2194, 0.1075, 0.3053, 0.0000, 0.0000],\n",
       "         [0.2058, 0.1239, 0.1531, 0.0000, 0.0141]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_word_embed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1069, 0.2424, 0.1167, 0.0000, 0.2654],\n",
       "         [0.2194, 0.1075, 0.3053, 0.0000, 0.0000],\n",
       "         [0.1323, 0.0992, 0.0000, 0.0000, 0.0519]],\n",
       "\n",
       "        [[0.2058, 0.1239, 0.1531, 0.0000, 0.0141],\n",
       "         [0.2194, 0.1075, 0.3053, 0.0000, 0.0000],\n",
       "         [0.2058, 0.1239, 0.1531, 0.0000, 0.0141]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_word_emb_unflatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [2, 2, 2],\n",
      "        [0, 0, 2]])\n",
      "tensor([[1, 1, 2],\n",
      "        [2, 2, 2],\n",
      "        [1, 1, 2]])\n",
      "tensor([[1, 1, 1],\n",
      "        [2, 2, 2],\n",
      "        [0, 0, 2]])\n",
      "tensor([[1, 1, 2],\n",
      "        [2, 2, 2],\n",
      "        [1, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "for y in torch.split(input, split_size_or_sections = 1):\n",
    "    Y_t = torch.squeeze(y, dim=0)\n",
    "    print(Y_t)\n",
    "\n",
    "\n",
    "\n",
    "for Y_t in input:\n",
    "    print(Y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, char_embedding_size=50, target_vocab=None):\n",
    "        \"\"\" Init Character Decoder.\n",
    "\n",
    "        @param hidden_size (int): Hidden size of the decoder LSTM\n",
    "        @param char_embedding_size (int): dimensionality of character embeddings\n",
    "        @param target_vocab (VocabEntry): vocabulary for the target language. See vocab.py for documentation.\n",
    "        \"\"\"\n",
    "        super(CharDecoder, self).__init__()\n",
    "        self.target_vocab = target_vocab\n",
    "        self.charDecoder = nn.LSTM(char_embedding_size, hidden_size)\n",
    "        self.char_output_projection = nn.Linear(hidden_size, 20)\n",
    "        self.decoderCharEmb = nn.Embedding(20, char_embedding_size,\n",
    "                                           padding_idx=0)\n",
    "\n",
    "    def forward(self, input, dec_hidden=None):\n",
    "        \"\"\" Forward pass of character decoder.\n",
    "\n",
    "        @param input (Tensor): tensor of integers, shape (length, batch_size)\n",
    "        @param dec_hidden (tuple(Tensor, Tensor)): internal state of the LSTM before reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)\n",
    "\n",
    "        @returns scores (Tensor): called s_t in the PDF, shape (length, batch_size, self.vocab_size)\n",
    "        @returns dec_hidden (tuple(Tensor, Tensor)): internal state of the LSTM after reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE for part 2a\n",
    "        ### TODO - Implement the forward pass of the character decoder.\n",
    "        x = self.decoderCharEmb(input)\n",
    "        dec_hidden, (last_hidden, last_cell) = self.charDecoder(x, dec_hidden)\n",
    "\n",
    "        scores = self.char_output_projection(dec_hidden)\n",
    "        return scores, (last_hidden, last_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = CharDecoder(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8395966027bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = torch.LongTensor([[[1,2,3,4],[1,2,0,0],[2,2,2,0]],[[1,2,3,0],[1,2,0,1],[2,2,2,2]]])\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sequence = torch.LongTensor([[1,0,0],[1,2,0]])\n",
    "char_sequence.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., inf, inf],\n",
       "        [0., 0., inf]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_masks = torch.zeros(2,3)\n",
    "dec_masks[char_sequence == 0] =  float('inf')\n",
    "dec_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8d30b5e54da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'long'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.LongTensor([[[1,2,3,4],[1,2,0,0],[2,2,2,0]],[[1,2,3,0],[1,2,0,1],[2,2,2,2]]])\n",
    "scores.size()\n",
    "# scores[:,:, 0] += dec_masks.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 1, 2],\n",
       "        [2, 1, 3]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(scores, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[                   1,                    2,                    3,\n",
       "                             4],\n",
       "         [-9223372036854775807,                    2,                    0,\n",
       "                             0],\n",
       "         [-9223372036854775806,                    2,                    2,\n",
       "                             0]],\n",
       "\n",
       "        [[                   1,                    2,                    3,\n",
       "                             0],\n",
       "         [                   1,                    2,                    0,\n",
       "                             1],\n",
       "         [-9223372036854775806,                    2,                    2,\n",
       "                             2]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 11])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(inputs)[1][1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.Tensor([[1,8888,1],[1,4444,1]])\n",
    "target = torch.LongTensor([0,0])\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6665.)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (target == 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(target==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sequence[1:].contiguous().view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sequence[1:].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[                   1,                    2,                    3,\n",
       "                             4],\n",
       "         [-9223372036854775807,                    2,                    0,\n",
       "                             0],\n",
       "         [-9223372036854775806,                    2,                    2,\n",
       "                             0]],\n",
       "\n",
       "        [[                   1,                    2,                    3,\n",
       "                             0],\n",
       "         [                   1,                    2,                    0,\n",
       "                             1],\n",
       "         [-9223372036854775806,                    2,                    2,\n",
       "                             2]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_char_tensor = torch.argmax(scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_char_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
